#### container info
image:
  singularity: nobrainer-zoo_nobrainer.sif
  docker: neuronets/nobrainer-zoo:nobrainer
  
#### repository info
repository:   
    repo_url: "git@github.com:neuronets/nobrainer.git"
    committish: "72aa211b"
    repo_download: False
    repo_download_location: "None"
    
#### required fields for prediction
inference:
    prediction_script: "nobrainer/nobrainer/cli/main.py"
    command: f"nobrainer predict -m {model_path} {infile[0]} {outfile}"
    # TODO: we should add help for model options
    options:
      block_shape: {mandatory: False, default: [128, 128, 128], argstr: "-b", type: "list"}
      resize_features_to: {mandatory: False, default: [256, 256, 256], argstr: "-r", type: "list"}
      threshold: {mandatory: False, default: 0.3, argstr: "-t", type: "float"}
      rotate_and_predict: {argstr: "--rotate-and-predict", is_flag: true}
      largest_label: {arstr: "-l", is_flag: true}
      verbose: {argstr: "-v", is_flag: true}
    #### input data characteristics
    data_spec:
      infile: {n_files: 1}
      outfile: {n_files: 1}
      
#### required fields for model training 
train:
    #### Train script
    train_script: Brainy_Train_Unet.py

    # the sample data used if data_patterns is not provided by the user
    sample_data: sample_MGH

    #### general settings
    name: unet_brainy
    is_train: true
    #use_visdom: false # for visualization
    #visdom_port: 8067  
    model: cnn
    device: cuda:0

    #### datasets
    n_classes: 1
    dataset_train:
      data_location: data/
      shuffle_buffer_size: 10
      block_shape: 32
      volume_shape: 256
      batch_size: 2  # per GPU
      augment: False
      n_train: 9
      num_parallel_calls: 2 # keeping same as batch sizse\
    dataset_test:     #  test params may differ from train params
      data_location: data/
      shuffle_buffer_size: 0
      block_shape: 32
      volume_shape: 256
      batch_size: 1
      n_test: 1
      num_parallel_calls: 1
      augment: False

    #### network structures
    network:
      model: unet
      batchnorm: True
    #### training settings: learning rate scheme, loss
    training:
      epoch: 5
      lr: .00001  # adam
      loss: nobrainer.losses.dice
      metrics: [nobrainer.metrics.dice, nobrainer.metrics.jaccard]

    #### logger
    logger:
      ckpt_path: ckpts/
  
    path:
      save_model: model/
      pretrained_model: none
      
#### training data characteristics
training_data_info:
    data_number:
        total: 10000
        train: None
        evaluate: None
        test: 99 
    biological_sex:
        male: None
        female: None
    age_histogram: None
    race: None
    imaging_contrast_info: "T1-weighted"
    dataset_sources: None
    data_sites:
        number_of_sites: None
        sites: None
    scanner_models: None
    hardware: None
    training_parameters:
        input_shape: "256x256x256"
        block_shape: "128x128x128" 
        n_classes: 1
        lr: None
        n_epochs: 5
        total_batch_size: 2
        number_of_gpus: None
        loss_function: "Jaccard"
        metrics: "Dice"
        data_preprocessing: "standard score"
        data_augmentation: "random rigid transformations applied to 50% of data"
        